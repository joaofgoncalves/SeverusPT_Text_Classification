---
title: "SeverusPT automated text clasification"
author: "JG, APP"
format: pdf
editor: visual
number-sections: true
number-depth: 3
---

# Text mining and classification of papers

The main objective for this task is to use examples of papers that are selected (or not) for each of the phases of wildfire research, namely:

-   Pre-fire
-   During-fire
-   Post-fire

Then a Random Forest model will be used to learn with these binary examples and classify all the remaining papers as selected or non-selected.

## Count functions

Load libs and define ancillary functions

```{r}
#| warning: false
#| error: false
#| message: false
#| echo: false

library(tidytext)
library(stringr)
library(dplyr)
library(caTools)
library(tidyr)
library(tidytext)
library(readr)
library(tidyverse)
library(randomForest)
library(Metrics)

auroc <- function(score, bool) {
  n1 <- sum(!bool)
  n2 <- sum(bool)
  U  <- sum(rank(score)[!bool]) - n1 * (n1 + 1) / 2
  return(1 - U / n1 / n2)
}

count_common_words <- function(title, common_words) {
  # tokenize the title into individual words
  words <- str_split(title, "\\s+")[[1]]
  
  # count the number of occurrences of each common word
  counts <- sapply(common_words, function(word) as.integer(sum(words == word) > 0))
  
  # return the vector of counts, with zeros included for words not found
  return(ifelse(is.na(counts), 0, counts))
}

count_common_keywords <- function(title, common_words) {
  
  # tokenize the title into individual words
  words <- str_split(gsub(";","",title), "\\s+")[[1]]
  
  # count the number of occurrences of each common word
  counts <- sapply(common_words, function(word) as.integer(sum(words == word) > 0))
  
  # return the vector of counts, with zeros included for words not found
  return(ifelse(is.na(counts), 0, counts))
}



```

## Data

Read the data and perform some basic preparation

Check colnames in the dataset

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


papers <- read_csv("pre_fire_title_screening_v2.csv") %>% 
  mutate(included = ifelse(screened_titles == "selected", 1, 0))

print(colnames(papers))

train_papers <- papers %>% 
  select(-1, -3) %>% 
  filter(!is.na(screened_titles)) %>% 
  filter(screened_titles %in% c("selected", "excluded")) %>% 
  mutate(included = ifelse(screened_titles == "selected", 1, 0))


```


Print class percentages:

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


tb <- table(train_papers$screened_titles)

knitr::kable((tb / sum(tb))*100, digits = 2)


```



## Training data

The training dataset is made by combining the following binary features,

-   Top-30 most common words/terms from the **title** in *selected papers*;

-   Top-30 most common words/terms from the **title** in *non-selected papers*;

-   Top-30 most common words/terms from the **keywords** in *selected papers*;

-   Top-30 most common words/terms from the **keywords** in *non-selected papers*;

### Step 1 - count the most frequent terms in the title and keywords

#### Title top-30 terms

Count words in titles (after removing common/stop words) and select those terms that are most common in selected/accepted papers and non-selected/non-accepted papers.

These terms will be used as binary features in RF classification.

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


# tokenize each title into individual words
words_tibble <- train_papers %>%
  select(label, title, included) %>% 
  unnest_tokens(word, title) %>% 
  anti_join(stop_words, by = join_by(word)) # remove very common terms


# group the words by acceptance and count their frequency
freq_tibble <- words_tibble %>%
  count(included, word, sort = TRUE)


# additional stuff to remove
to_rm <- c("ð", "â", "based", "due", 1:10)


# get the 30 most common words for accepted papers
common_words_selected_papers <- freq_tibble %>%
  filter(included == 1) %>% 
  filter(!is.na(word)) %>% 
  filter(!(word %in% to_rm)) %>% 
  slice(1:30)


# get the 30 most common words for non-accepted papers
common_words_unselected_papers <- freq_tibble %>%
  filter(included == 0) %>% 
  filter(!(word %in% common_words_selected_papers$word)) %>% 
  filter(!is.na(word)) %>% 
  filter(!(word %in% to_rm)) %>% 
  arrange(desc(n)) %>% 
  slice(1:30)

```

List of common terms in *titles* for *selected papers*:

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


knitr::kable(common_words_selected_papers)
```

List of common terms in *titles* for *non-selected papers*:

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


knitr::kable(common_words_unselected_papers)
```


------------------------------------------------------------------------

#### Keywords top-30 terms

Now repeat the same process but now using keywords:

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


# tokenize each keyword list into individual words
keywords_tibble <- train_papers %>%
  select(label, keywords, included) %>% 
  unnest_tokens(keyword, keywords) %>%
  anti_join(stop_words, by=c("keyword"="word"))


# group the words by acceptance and count their frequency
keywords_freq_tibble <- keywords_tibble %>%
  count(included, keyword, sort = TRUE)


# get the 30 most common keywords for non-accepted papers
common_keywords_selected_papers <- keywords_freq_tibble %>%
  filter(included == 1) %>% 
  filter(!is.na(keyword)) %>% 
  filter(!(keyword %in% to_rm)) %>% 
  # Remove terms already selected from the title
  #filter(!(keyword %in% common_words_selected_papers$word)) %>% 
  #filter(!(keyword %in% common_words_unselected_papers$word)) %>% 
  slice(1:30)


# get the 30 most common keywords for non-accepted papers
common_keywords_unselected_papers <- keywords_freq_tibble %>%
  filter(included == 0) %>% 
  # Remove terms already selected before
  filter(!(keyword %in% common_keywords_selected_papers$keyword)) %>% 
  # Remove terms already selected from the title
  #filter(!(keyword %in% common_words_selected_papers$word)) %>% 
  #filter(!(keyword %in% common_words_unselected_papers$word)) %>% 
  filter(!is.na(keyword)) %>% 
  filter(!(keyword %in% to_rm)) %>% 
  arrange(desc(n)) %>% 
  slice(1:30)

```


List of common terms in *keywords* for *selected papers*:

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


knitr::kable(common_keywords_selected_papers)
```

List of common terms in *keywords* for *non-selected papers*:

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


knitr::kable(common_keywords_unselected_papers)
```



------------------------------------------------------------------------

#### Abstract top-30 terms

Now repeat the same process but now using the abstract:

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


# tokenize each keyword list into individual words
abstract_tibble <- train_papers %>%
  select(label, abstract, included) %>% 
  unnest_tokens(abstract_, abstract) %>%
  anti_join(stop_words, by=c("abstract_"="word"))


# group the words by acceptance and count their frequency
abstract_freq_tibble <- abstract_tibble %>%
  count(included, abstract_, sort = TRUE)


# get the 30 most common keywords for non-accepted papers
common_abstract_selected_papers <- abstract_freq_tibble %>%
  filter(included == 1) %>% 
  filter(!is.na(abstract_)) %>% 
  filter(!(abstract_ %in% to_rm)) %>%  
  slice(1:50)


# get the 30 most common keywords for non-accepted papers
common_abstract_unselected_papers <- abstract_freq_tibble %>%
  filter(included == 0) %>% 
  # Remove terms already selected before
  filter(!(abstract_ %in% common_abstract_selected_papers$abstract_)) %>% 
  filter(!is.na(abstract_)) %>% 
  filter(!(abstract_ %in% to_rm)) %>% 
  arrange(desc(n)) %>% 
  slice(1:50)

```

List of common terms in *abstracts* for *selected papers*:

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


knitr::kable(common_abstract_selected_papers)
```

List of common terms in *abstracts* for *non-selected papers*:

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


knitr::kable(common_abstract_unselected_papers)
```



### Step 2 - make features

Count the selected words in the title and keywords and arrange them as a binary grid:

#### Title-based features

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


# Title words

# create a new tibble with the counts for each of the top 30 words in titles
counts_sel_words <- t(apply(as.data.frame(train_papers %>% select(title)), 1, count_common_words, 
                         common_words = common_words_selected_papers$word)) %>% 
  as.data.frame() %>% 
  `colnames<-`(paste0("words_sel_", colnames(.)))

# create a new tibble with the counts for each of the top 30 words
counts_unsel_words <- t(apply(as.data.frame(train_papers %>% select(title)), 1, count_common_words, 
                            common_words = common_words_unselected_papers$word)) %>% 
  as.data.frame() %>% 
  `colnames<-`(paste0("words_nsel_", colnames(.)))

```

#### Keyword-based features

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


# Keywords

# create a new tibble with the counts for each of the top 30 keywords
counts_sel_keywords <- t(apply(as.data.frame(train_papers %>% select(keywords)), 1, count_common_keywords, 
                            common_words = common_keywords_selected_papers$keyword)) %>% 
  as.data.frame() %>% 
  `colnames<-`(paste0("keywords_sel_", colnames(.)))

# create a new tibble with the counts for each of the top 30 keywords
counts_unsel_keywords <- t(apply(as.data.frame(train_papers %>% select(keywords)), 1, count_common_keywords, 
                              common_words = common_keywords_unselected_papers$keyword)) %>% 
  as.data.frame() %>% 
  `colnames<-`(paste0("keywords_nsel_", colnames(.)))
```


#### Abstract-based features

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


# Abstract

# create a new tibble with the counts for each of the top 30 keywords
counts_sel_abstract <- t(apply(as.data.frame(train_papers %>% select(abstract)), 1, count_common_words, 
                            common_words = common_abstract_selected_papers$abstract_)) %>% 
  as.data.frame() %>% 
  `colnames<-`(paste0("abstract_sel_", colnames(.)))

# create a new tibble with the counts for each of the top 30 keywords
counts_unsel_abstract <- t(apply(as.data.frame(train_papers %>% select(abstract)), 1, count_common_words, 
                              common_words = common_abstract_unselected_papers$abstract_)) %>% 
  as.data.frame() %>% 
  `colnames<-`(paste0("abstract_nsel_", colnames(.)))
```




### Step3 - assemble all features and labels

Make the training dataset by combining everything:

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


# combine the counts with the original titles and labels
train_tb <- bind_cols(train_papers %>% select(label, included), 
                      counts_sel_words, 
                      counts_unsel_words,
                      counts_sel_keywords,
                      counts_unsel_keywords,
                      counts_sel_abstract,
                      counts_unsel_abstract)

#cat("Training dataset:\n\n")
#knitr::kable(train_tb %>% slice(1:10) %>% t)

```



## Prediction dataset

Step 1 - Evaluate the same features as before but now for the entire dataset of papers

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


# This part prepares the prediction dataset to use the RF model

# create a new tibble with the counts for each of the top 30 words
counts_sel_words_all <- t(apply(as.data.frame(papers %>% select(title)), 1, count_common_words, 
                            common_words = common_words_selected_papers$word)) %>% 
  as.data.frame() %>% 
  `colnames<-`(paste0("words_sel_", colnames(.)))

# create a new tibble with the counts for each of the top 30 words
counts_unsel_words_all <- t(apply(as.data.frame(papers %>% select(title)), 1, count_common_words, 
                              common_words = common_words_unselected_papers$word)) %>% 
  as.data.frame() %>% 
  `colnames<-`(paste0("words_nsel_", colnames(.)))


## ------------------------------------------------------------------------------------------------ ##


# create a new tibble with the counts for each of the top 30 keywords
counts_sel_keywords_all <- t(apply(as.data.frame(papers %>% select(keywords)), 1, count_common_keywords, 
                               common_words = common_keywords_selected_papers$keyword)) %>% 
  as.data.frame() %>% 
  `colnames<-`(paste0("keywords_sel_", colnames(.)))

# create a new tibble with the counts for each of the top 30 keywords
counts_unsel_keywords_all <- t(apply(as.data.frame(papers %>% select(keywords)), 1, count_common_keywords, 
                                 common_words = common_keywords_unselected_papers$keyword)) %>% 
  as.data.frame() %>% 
  `colnames<-`(paste0("keywords_nsel_", colnames(.)))



## ------------------------------------------------------------------------------------------------ ##


# create a new tibble with the counts for each of the top 30 keywords
counts_sel_abstract_all <- t(apply(as.data.frame(papers %>% select(abstract)), 1, count_common_words, 
                               common_words = common_abstract_selected_papers$abstract_)) %>% 
  as.data.frame() %>% 
  `colnames<-`(paste0("abstract_sel_", colnames(.)))

# create a new tibble with the counts for each of the top 30 keywords
counts_unsel_abstract_all <- t(apply(as.data.frame(papers %>% select(abstract)), 1, count_common_words, 
                                 common_words = common_abstract_unselected_papers$abstract_)) %>% 
  as.data.frame() %>% 
  `colnames<-`(paste0("abstract_nsel_", colnames(.)))


```

Step 2 - Assemble the full prediction dataset:

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


# combine the counts with the original titles and labels
pred_tb <- bind_cols(papers %>% select(label, included), 
                      # Title words
                      counts_sel_words_all, 
                      counts_unsel_words_all,
                      # Keywords
                      counts_sel_keywords_all,
                      counts_unsel_keywords_all,
                      # Abstract
                      counts_sel_abstract_all,
                      counts_unsel_abstract_all
                     )

#cat("Prediction dataset:\n\n")
#knitr::kable(pred_tb %>% slice(1:10) %>% t)

```



## Random Forest model development

Make the classification model based on Random Forests:

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


## Full model
mtry_vals <- 3:10
rfs<-list()
acc_vals <- c()
i<-0
pb <- txtProgressBar(1, length(mtry_vals), style=3)

for (mtry_val in mtry_vals){
  i<-i+1
  rfs[[i]] <- randomForest(x = train_tb %>% select(-label, -included),
             y = train_tb %>% pull(included) %>% as.factor(), 
             ntree=500, mtry = mtry_val)
  
  conf <- rfs[[i]]$confusion[1:2, 1:2]
  acc_vals[i] <- sum(diag(conf))/sum(conf)
  
  setTxtProgressBar(pb,i)  
}


best_mtry <- mtry_vals[which.max(acc_vals)]

cat("best mtry value:\n\n")
print(best_mtry)


rf <- randomForest(x = train_tb %>% select(-label, -included),
             y = train_tb %>% pull(included) %>% as.factor(), 
             ntree=500, mtry = best_mtry)

print(rf)

plot(rf)

vimp <- importance(rf) %>% 
  as.data.frame() %>% 
  mutate(var_name = rownames(.)) %>% 
  arrange(desc(MeanDecreaseGini))

imp_vars <- vimp$var_name[1:50]


```

List of the top-50 features by decreasing order of Mean Decrease in Gini Index:

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


knitr::kable(data.frame(vimp %>% slice(1:50) %>% 
                          `rownames<-`(NULL)), digits = 2)

```


Re-train the model but now with the top-50 best set of features based on the importance rank:

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


## Selected best var set model
rf1 <- randomForest(x = train_tb %>% select(all_of(imp_vars)),
                   y = train_tb %>% pull(included) %>% as.factor(), 
                   ntree=500, mtry = best_mtry)

print(rf1)

plot(rf1)

```

Optimized cut-off to binarize the results:

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


pred <- predict(rf, type = "prob")[,2]

obs <- train_tb %>% pull(included)

thresh_seq <- seq(0.001, 1, by=0.001)

metrics_matrix <- matrix(NA, nrow=length(thresh_seq), ncol= 4)
colnames(metrics_matrix) <- c("thresh","auc","recall","prec")
  
for(i in seq_along(thresh_seq)){
  
  metrics_matrix[i,1] <- thresh_seq[i]
  metrics_matrix[i,2] <- auroc(obs, as.integer(pred >= thresh_seq[i]))
  metrics_matrix[i,3] <- recall(obs, as.integer(pred >= thresh_seq[i]))
  metrics_matrix[i,4] <- precision(obs, as.integer(pred >= thresh_seq[i]))
}

best_cutoff <- metrics_matrix %>% as.data.frame() %>% arrange(desc(auc)) %>% slice(1)

knitr::kable(best_cutoff, digits = 3)

```

Predict class labels for the entire dataset using the optimized cut-off.

```{r}
#| error: false
#| warning: false
#| message: false
#| echo: false


pred_prob <- predict(rf, newdata = pred_tb %>% select(-label, -included), type="prob")[,2]

pred_class <- as.integer(pred_prob >= best_cutoff$thresh[1])

pclass_tb <- table(pred_class)


cat("Predicted class percentages:\n\n")
print(round( ((pclass_tb / sum(pclass_tb))*100), 1))

```
